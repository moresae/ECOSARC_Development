# -*- coding: utf-8 -*-
"""unet_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uc0obFicJRgnn5jGvvu4Mq361goDKjQ6
"""

import os
from reprlib import aRepr
# for reading and processing images
import imageio
import matplotlib.pyplot as plt
import numpy as np # for using np arrays
from numpy import asarray
import cv2
import tensorflow as tf
from tensorflow.keras import layers
import os

"""Visualize/explore the dataset of images and masks"""

np.random.seed(42) 



def show_image_mask(img_view,mask_view):
  fig, arr = plt.subplots(1, 2, figsize=(5, 5))
  arr[0].imshow(img_view)
  arr[0].set_title('Image ' )
  arr[1].imshow(mask_view)
  arr[1].set_title('Masked Image ')


"""  LOAD AND SCALE DATA  """

Xx = np.load("X.npy")
yy = np.load("Y.npy")


def scale_dataset(i_h,i_w):
  m,a,b=Xx.shape
  X = np.zeros((m,i_h,i_w), dtype=np.float32)
  y = np.zeros((m,i_h,i_w), dtype=np.float32)

  i=0
  for img in Xx:
    scaled = cv2.resize(img,dsize=(i_w,i_h), interpolation=cv2.INTER_LINEAR) 
    X[i] =scaled/255
    i=i+1
  i=0
  for mask in yy:
    scaled = cv2.resize(mask,dsize=(i_w,i_h), interpolation=cv2.INTER_LINEAR) 
    y[i] =scaled/255
    i=i+1

  return X,y

#X,y = scale_dataset(400,256)
Xs,ys = scale_dataset(256,400)


print('X and y ready scaled')

## ------------------------------------------------------------------------------------------------


import time
# for bulding and running deep learning model
from tensorflow import keras
from tensorflow.keras import layers

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import Input
from tensorflow.keras.layers import MaxPooling2D
from tensorflow.keras.layers import Dropout 
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.layers import Conv2DTranspose
from tensorflow.keras.layers import concatenate
from tensorflow.keras.losses import binary_crossentropy
from tensorflow.keras.layers import UpSampling2D
import pandas as pd
from tensorflow import keras
from keras import backend as K
#---------------------------------

def unet_1(pretrained_weights = None, input_shape = (400,256)):
    inputs = tf.keras.Input(input_shape)
    #inputs2 =  tf.keras.layers.Rescaling(1.0 / 255, offset=0.0)(inputs)
    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)
    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)
    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)
    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)
    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)
    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)
    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)
    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)
    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)
    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)
    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)
    drop4 = Dropout(0.5)(conv4)
    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)
    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)
    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)
    drop5 = Dropout(0.5)(conv5)
    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))
    merge6 = concatenate([drop4,up6], axis = 3)
    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)
    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)
    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))
    merge7 = concatenate([conv3,up7], axis = 3)
    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)
    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)
    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))
    merge8 = concatenate([conv2,up8], axis = 3)
    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)
    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)
    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))
    merge9 = concatenate([conv1,up9], axis = 3)
    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)
    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)
    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)
    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)
    model = Model(inputs = inputs, outputs = conv10)


    #model.summary()
    if(pretrained_weights):
        model.load_weights(pretrained_weights)
    return model


def dice_coef2p0(y_true, y_pred, smooth=0):
    #
    #Dice = (2*|X & Y|)/ (|X|+ |Y|)
    #     =  2*sum(|A*B|)/(sum(A^2)+sum(B^2))
    #ref: https://arxiv.org/pdf/1606.04797v1.pdf
    #
    intersection = K.sum(K.abs(y_true * y_pred))
    return (2. * intersection + smooth) / (K.sum(K.abs(y_true)) + K.sum(K.abs(y_pred)) + smooth)
def dice_coef_loss(y_true, y_pred):
    return 1-dice_coef2p0(y_true, y_pred)

model = unet_1(pretrained_weights = None, input_shape = (256, 400,1))
#model = unet_1(pretrained_weights = None, input_shape = (400, 256,1))
#model.summary()


#--------------------------------------------------------------------
# Use k-fold validation. The size of the full dataset 
# (without augmentation is 529 images-masks)
#--------------------------------------------------------------------
#--------------------------------------------------------------------
from sklearn.model_selection import KFold, train_test_split
"""SPLIT DATA INTO TRAIN-VALIDATION-TEST"""


#Divide dataset in train+val ---- test
x_train_val, x_test, y_train_val, y_test = train_test_split(Xs,ys, test_size=0.1, random_state=42,shuffle=False)
#Leave 10% of the data to then test the model with new data, unseen for the model
#x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2

print("shape of x_train_val")
print(x_train_val.shape)

       
kf = KFold(n_splits=5,random_state=42, shuffle=True)
#kf.get_n_splits(X)

k_iter = 0


num_epochs = 10

nsamples, nx, ny = x_train_val.shape
d2_x_train_val = x_train_val.reshape((nsamples,nx*ny))

nsamples, nx, ny = y_train_val.shape
d2_y_train_val = y_train_val.reshape((nsamples,nx*ny))


#------------------------------------------------
#         K-FOLD TRAIN-VAL PROCEDURE.
#------------------------------------------------
for train_index, val_index in kf.split(d2_x_train_val, d2_y_train_val):

  k_iter+=1
  print(' K = '+str(k_iter))
  # COMPILE NEW MODEL
  model.compile(optimizer = keras.optimizers.Adam(learning_rate=1e-4),
              loss=dice_coef_loss,
              metrics = ['accuracy'])
              
  callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5) #early stopping
  x_train_fold = x_train_val[train_index]
  print("shape of x_train_fold")
  print(x_train_fold.shape)

	# There can be other callbacks, but just showing one because it involves the model name
	# This saves the best model
	# AUGMENT DATA FOR TRAINING
  x_fold = x_train_val[train_index]
  y_fold = y_train_val[train_index]


  X_augm = x_fold
  y_augm = y_fold

	# AUGMENT DATA FOR VALIDATION
  x_fold_val = x_train_val[val_index]
  y_fold_val = y_train_val[val_index]
  na =25

  X_augm_val = x_fold_val
  y_augm_val = y_fold_val

  history = model.fit(X_augm,y_augm,
        	    epochs=num_epochs,
                batch_size=10,
			    callbacks=[callback],
			    validation_data=(X_augm_val,y_augm_val))
	


  # list all data in history
  print(history.history.keys())
  # summarize history for accuracy
  plt.plot(history.history['accuracy'])
  plt.plot(history.history['val_accuracy'])
  plt.title('model accuracy')
  plt.ylabel('accuracy')
  plt.xlabel('epoch')
  plt.legend(['train', 'val'], loc='upper left')
  #plt.show()
  plt.savefig('./results_unet1_aug/'+'acc_k_'+str(k_iter)+'.png',format='png')
  plt.close()

  # summarize history for loss
  plt.plot(history.history['loss'])
  plt.plot(history.history['val_loss'])
  plt.title('model loss')
  plt.ylabel('loss')
  plt.xlabel('epoch')
  plt.legend(['train', 'val'], loc='upper left')
  plt.savefig('./results_unet1/'+'loss_k_'+str(k_iter)+'.png',format='png')
  plt.close()


  results = model.evaluate(X_augm_val,y_augm_val)
  results = dict(zip(model.metrics_names,results))

  # PREDICT ON *VALIDATION* DATA
  yp_val = model.predict(X_augm_val)
 
  #save_name = "augm_y_pred_xval_k_"+str(k_iter)+".npy"
  #np.save(save_name, yp_val)

  loss, accuracy = model.evaluate(x_train_val[val_index],y_train_val[val_index])
  print("RESTULS WITH VALIDATION DATA")
  print("Loss: ", loss)
  print("Accuracy: ", accuracy)
  print("-------------------------------------------------------")


  loss, accuracy = model.evaluate(x_test,y_test)
  print("RESTULS WITH TEST DATA")
  print("Loss: ", loss)
  print("Accuracy: ", accuracy)
  print("-------------------------------------------------------")
  
  #save each k-model
  # Calling `save('my_model')` creates a SavedModel folder `my_model`.
  model_name = 'model_'+str(k_iter)
  model.save(model_name)
  
  # PREDICT ON *TEST* DATA. This dataset has NOT been used to train the model
  yp = model.predict(x_test)

  save_name = "y_pred_xtest_k_"+str(k_iter)+".npy"
  np.save(save_name, yp)



  if k_iter == 2:
    n_images_save = 8*4
    for i in range(n_images_save):
      plt.figure(figsize=(20,10))
      plt.subplot(1,3,1)
      plt.imshow(x_test[i])
      plt.title('Input')
      plt.subplot(1,3,2)
      plt.imshow(y_test[i].reshape(y_test[i].shape[0],y_test[i].shape[1]))
      plt.title('Ground Truth')
      plt.subplot(1,3,3)
      plt.imshow(yp[i].reshape(yp[i].shape[0],yp[i].shape[1]))
      plt.title('Prediction')
      plt.savefig('./results_unet1/'+str(i)+'.png',format='png')
      plt.close()















