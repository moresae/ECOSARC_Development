# -*- coding: utf-8 -*-
"""unet_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uc0obFicJRgnn5jGvvu4Mq361goDKjQ6
"""

import os
from reprlib import aRepr
# for reading and processing images
import imageio
import matplotlib.pyplot as plt
import numpy as np # for using np arrays
from numpy import asarray
import cv2
import tensorflow as tf
from tensorflow.keras import layers
import os

"""Visualize/explore the dataset of images and masks"""

def show_image_mask(img_view,mask_view):
  fig, arr = plt.subplots(1, 2, figsize=(5, 5))
  arr[0].imshow(img_view)
  arr[0].set_title('Image ' )
  arr[1].imshow(mask_view)
  arr[1].set_title('Masked Image ')


"""  LOAD AND SCALE DATA  """

Xx = np.load("X.npy")
yy = np.load("Y.npy")


def scale_dataset(i_h,i_w):
  m,a,b=Xx.shape
  X = np.zeros((m,i_h,i_w), dtype=np.float32)
  y = np.zeros((m,i_h,i_w), dtype=np.float32)

  i=0
  for img in Xx:
    scaled = cv2.resize(img,dsize=(i_w,i_h), interpolation=cv2.INTER_LINEAR) 
    X[i] =scaled/255
    i=i+1
  i=0
  for mask in yy:
    scaled = cv2.resize(mask,dsize=(i_w,i_h), interpolation=cv2.INTER_LINEAR) 
    y[i] =scaled/255
    i=i+1

  return X,y

#X,y = scale_dataset(400,256)
X,y = scale_dataset(256,400)


print('X and y ready scaled')

## ------------------------------------------------------------------------------------------------


"""SPLIT DATA INTO TRAIN-VALIDATION-TEST"""
from sklearn.model_selection import train_test_split

#Divide dataset in train-val-test
x_train, x_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=1)
x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2

print(x_train.shape)
#print(x_val.shape)
#print(x_test.shape)

"""#   DATA AUGMENTATION ON THE TRAINING DATASET"""
def augment_image(img):
  im1 =scale_horizontal(img,10)
  im2 =scale_horizontal(img,-10)
  im3 =scale_vertical(img,15)
  im4 =scale_vertical(img,-15)
  im5 =horizontal_mirror(img)
  im6 = change_brightness(img,0.5)
  im7 = change_brightness(img,0.8)
  return im1,im2,im3,im4,im5,im6,im7

def scale_vertical(img,factor):
  new_height = int(img.shape[0]*(1+factor/100))
  dim_size = (img.shape[1],new_height)
  vertical_img = cv2.resize(img, dim_size, interpolation=cv2.INTER_LINEAR)
  return vertical_img

# horizontal scaling
def scale_horizontal(img,factor):
  new_width = int(img.shape[1]*(1+factor/100))
  dim_size = (new_width,img.shape[0])
  horizon_img = cv2.resize(img, dim_size, interpolation=cv2.INTER_LINEAR)
  return horizon_img

def horizontal_mirror(img):
  return np.fliplr(img)

def change_brightness(image,gain):
   new_image = tf.image.adjust_brightness(image, gain)
   return new_image.numpy()


def image_augment(na,i_h,i_w):
  # Pull the relevant dimensions for image and mask
  m,a,b = x_train.shape  # pull height, width, and channels of image

  # Define X and Y as number of images along with shape of one image
  Xa = np.zeros((m*na,i_h,i_w), dtype=np.float32)
  ya = np.zeros((m*na,i_h,i_w), dtype=np.float32)

  i = 0 
  for single_img in x_train:
    im1,im2,im3,im4,im5,im6,im7 = augment_image(single_img)

    Xa[i]= cv2.resize(im1, dsize=(i_w,i_h), interpolation=cv2.INTER_LINEAR) 
    i+=1
    
    Xa[i]=cv2.resize(im2,dsize=(i_w,i_h), interpolation=cv2.INTER_LINEAR) 
    i+=1 
    
    Xa[i]=cv2.resize(im3,dsize=(i_w,i_h), interpolation=cv2.INTER_LINEAR) 
    i+=1
    
    Xa[i]=cv2.resize(im4, dsize=(i_w,i_h), interpolation=cv2.INTER_LINEAR) 
    i+=1 
    
    Xa[i]=im5
    i+=1

    Xa[i]=im6
    i+=1

    Xa[i]=im7
    i+=1

  i = 0    
  for single_mask in y_train:
    
    im1,im2,im3,im4,im5,im6,im7 = augment_image(single_mask)

    ya[i]=cv2.resize(im1, dsize=(i_w,i_h), interpolation=cv2.INTER_LINEAR) 
    i+=1
    
    ya[i]=cv2.resize(im2,dsize=(i_w,i_h), interpolation=cv2.INTER_LINEAR) 
    i+=1 
    
    ya[i]=cv2.resize(im3, dsize=(i_w,i_h), interpolation=cv2.INTER_LINEAR)  
    i+=1
    
    ya[i]=cv2.resize(im4,dsize=(i_w,i_h), interpolation=cv2.INTER_LINEAR)  
    i+=1 
    
    ya[i]=im5
    i+=1

    ya[i]=im6
    i+=1

    ya[i]=im7
    i+=1   

       
  return Xa,ya


na = 7
#Xa,ya = image_augment(na,400, 256)
Xa,ya = image_augment(na,256, 400)


#np.save('Xa_train.npy', Xa)
#np.save('ya_train .npy', ya )

#Train dataset: add the augmentnted data
X_train_augm = np.concatenate((x_train,Xa))
y_train_augm = np.concatenate((y_train,ya))



# Start from here with data loaded



import time
# for bulding and running deep learning model
from tensorflow import keras
from tensorflow.keras import layers

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import Input
from tensorflow.keras.layers import MaxPooling2D
from tensorflow.keras.layers import Dropout 
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.layers import Conv2DTranspose
from tensorflow.keras.layers import concatenate
from tensorflow.keras.losses import binary_crossentropy
from tensorflow.keras.layers import UpSampling2D


def unet_1(pretrained_weights = None, input_shape = (400,256)):
    inputs = tf.keras.Input(input_shape)
    #inputs2 =  tf.keras.layers.Rescaling(1.0 / 255, offset=0.0)(inputs)
    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)
    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)
    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)
    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)
    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)
    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)
    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)
    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)
    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)
    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)
    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)
    drop4 = Dropout(0.5)(conv4)
    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)
    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)
    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)
    drop5 = Dropout(0.5)(conv5)
    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))
    merge6 = concatenate([drop4,up6], axis = 3)
    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)
    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)
    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))
    merge7 = concatenate([conv3,up7], axis = 3)
    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)
    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)
    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))
    merge8 = concatenate([conv2,up8], axis = 3)
    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)
    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)
    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))
    merge9 = concatenate([conv1,up9], axis = 3)
    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)
    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)
    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)
    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)
    model = Model(inputs = inputs, outputs = conv10)


    #model.compile(optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])
    #model.summary()
    if(pretrained_weights):
        model.load_weights(pretrained_weights)
    return model

from keras import backend as K
def dice_coef2p0(y_true, y_pred, smooth=0):
    #
    #Dice = (2*|X & Y|)/ (|X|+ |Y|)
    #     =  2*sum(|A*B|)/(sum(A^2)+sum(B^2))
    #ref: https://arxiv.org/pdf/1606.04797v1.pdf
    #
    intersection = K.sum(K.abs(y_true * y_pred))
    return (2. * intersection + smooth) / (K.sum(K.abs(y_true)) + K.sum(K.abs(y_pred)) + smooth)
def dice_coef_loss(y_true, y_pred):
    return 1-dice_coef2p0(y_true, y_pred)

#img_out = np.load('img_out.npy');
#mask_out = np.load('mask_out.npy');

"""SHUFFLE THE DATA - ASSIGN INDICES"""
model = unet_1(pretrained_weights = None, input_shape = (256, 400,1))

#model = unet_1(pretrained_weights = None, input_shape = (400, 256,1))
#model.summary()


import pandas as pd
from tensorflow import keras

print("Sizes")
print(X_train_augm.shape)
print(y_train_augm.shape)

print(y_val.shape)
print(y_test.shape)

model.compile(optimizer = keras.optimizers.Adam(learning_rate=1e-4),
              loss=dice_coef_loss,
              metrics = ['accuracy'])#lr = 1e-4 ‘binary_crossentropy’
#loss='binary_crossentropy',
t = time.localtime()
ts = time.strftime('%b-%d-%Y_%H%M', t)


callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5) #early stopping

model_history = model.fit(X_train_augm, y_train_augm, validation_data=(x_val, y_val), epochs=5, batch_size=10, callbacks=[callback]) #,class_weight= w)

"""RESULTS"""
"""SHOW PREDICTIONS"""

loss, accuracy = model.evaluate(x_test,y_test)

print("Loss: ", loss)
print("Accuracy: ", accuracy)

# Evaluate the model on the test data using `evaluate`
print("Evaluate on test data")
results = model.evaluate(x_test, y_test)
print("test loss, test acc:", results)

# Generate predictions (probabilities -- the output of the last layer)
# on new data using `predict`
print("Generate predictions for samples")
yp = model.predict(x_test)


print("predictions shape:", yp.shape[0])

n_images_save = int(yp.shape[0])

for i in range(n_images_save):
  if (i % 2) == 0: 
    plt.figure(figsize=(20,10))
    plt.subplot(1,3,1)
    plt.imshow(x_test[i])
    plt.title('Input')
    plt.subplot(1,3,2)
    plt.imshow(y_test[i].reshape(y_test[i].shape[0],y_test[i].shape[1]))
    plt.title('Ground Truth')
    plt.subplot(1,3,3)
    plt.imshow(yp[i].reshape(yp[i].shape[0],yp[i].shape[1]))
    plt.title('Prediction')

    plt.savefig('./results_unet1/'+str(i)+'.png',format='png')
    plt.close()

"""test 2: epochs=10, batch_size=5

```
# This is formatted as code
```

, learning_rate=1e-4
"""